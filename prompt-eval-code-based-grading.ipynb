{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30e50d2",
   "metadata": {},
   "source": [
    "Generating test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c036bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb6d67",
   "metadata": {},
   "source": [
    "When evaluating AI models that generate code, you need more than just checking if the response makes sense. You also need to verify that the generated code actually has valid syntax and follows the correct format. This is where code-based grading comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9453c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"claude-sonnet-4-0\"\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50102c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences\n",
    "    }\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "    if stop_sequences:\n",
    "        params[\"stop_sequences\"] = stop_sequences\n",
    "    \n",
    "    response = client.messages.create(**params)\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2169a26",
   "metadata": {},
   "source": [
    "Code Based Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a46784e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "648f7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "995a59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Test Data with Code\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate an evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects, each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"task\": \"Description of task\",\n",
    "    \"format\" : \"python\" or \"json\" or \"regex\",\n",
    "  },\n",
    "  ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a single regex\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eedfa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task': \"Create a Python function that parses an S3 bucket ARN and returns the bucket name. The function should extract the bucket name from ARNs in the format 'arn:aws:s3:::bucket-name'\", 'format': 'python'}, {'task': 'Create a JSON configuration object for an AWS Lambda function that runs Python 3.9, has 512MB memory, 30 second timeout, and includes environment variables for DATABASE_URL and LOG_LEVEL', 'format': 'json'}, {'task': 'Write a regex pattern that matches valid AWS region names (e.g., us-east-1, eu-west-2, ap-southeast-1) which follow the format of 2-3 lowercase letters, hyphen, cardinal direction, hyphen, and a number', 'format': 'regex'}]\n"
     ]
    }
   ],
   "source": [
    "# Generate the dataset\n",
    "dataset = generate_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10698e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to a JSON file\n",
    "with open(r'dataset/test-case-dataset2.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e51af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with Python, JSON, or a plain Regex\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d85856b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c96056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "977553c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function coordinates the entire evaluation process:\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "    \n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071645e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8.0\n"
     ]
    }
   ],
   "source": [
    "with open(r\"dataset/test-case-dataset2.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c099edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\ndef parse_s3_bucket_arn(arn):\\n    return arn.split(':::')[1]\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a Python function that parses an S3 bucket ARN and returns the bucket name. The function should extract the bucket name from ARNs in the format 'arn:aws:s3:::bucket-name'\",\n",
      "      \"format\": \"python\"\n",
      "    },\n",
      "    \"score\": 7.0,\n",
      "    \"reasoning\": \"While the core logic works for valid S3 ARNs, the solution lacks robustness. It will fail catastrophically on invalid inputs without proper error handling or validation, making it unsuitable for production use.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"FunctionName\\\": \\\"my-lambda-function\\\",\\n  \\\"Runtime\\\": \\\"python3.9\\\",\\n  \\\"Role\\\": \\\"arn:aws:iam::123456789012:role/lambda-execution-role\\\",\\n  \\\"Handler\\\": \\\"lambda_function.lambda_handler\\\",\\n  \\\"Code\\\": {\\n    \\\"ZipFile\\\": \\\"\\\"\\n  },\\n  \\\"Description\\\": \\\"\\\",\\n  \\\"Timeout\\\": 30,\\n  \\\"MemorySize\\\": 512,\\n  \\\"Environment\\\": {\\n    \\\"Variables\\\": {\\n      \\\"DATABASE_URL\\\": \\\"\\\",\\n      \\\"LOG_LEVEL\\\": \\\"\\\"\\n    }\\n  }\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON configuration object for an AWS Lambda function that runs Python 3.9, has 512MB memory, 30 second timeout, and includes environment variables for DATABASE_URL and LOG_LEVEL\",\n",
      "      \"format\": \"json\"\n",
      "    },\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The solution correctly implements the basic Lambda configuration structure and meets all specified requirements. However, it uses placeholder values that would need to be replaced for real deployment, making it more of a template than a production-ready configuration.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n^[a-z]{2,3}-(?:north|south|east|west|northeast|northwest|southeast|southwest|central)-[0-9]+$\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a regex pattern that matches valid AWS region names (e.g., us-east-1, eu-west-2, ap-southeast-1) which follow the format of 2-3 lowercase letters, hyphen, cardinal direction, hyphen, and a number\",\n",
      "      \"format\": \"regex\"\n",
      "    },\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The regex captures the general AWS region format well but lacks precision. Real AWS regions use specific patterns: typically 1-2 digits, specific region prefixes, and limited direction terms. While functionally correct for basic validation, it's overly broad for production use.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Examining the Results\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f26958",
   "metadata": {},
   "source": [
    "Each result contains three key pieces of information:\n",
    "\n",
    "- output: The complete response from Claude\n",
    "- test_case: The original test case that was processed\n",
    "- score: The evaluation score (currently hardcoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
